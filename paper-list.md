# Paper List

[toc]

## Alignment

### Survey

#### Aligning Large Language Models with Human: A Survey

#### From Instructions to Intrinsic Human Values —— A Survey of Alignment Goals for Big Models



### Algorithm

#### RLHF：Training language models to follow instructions with human feedback

#### DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model

#### RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment

TOREAD:

RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback

LIMA: Less Is More for Alignment

SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions

Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision

Training Socially Aligned Language Models in Simulated Human Society

RRHF: Rank Responses to Align Language Models with Human Feedback without tears

PRO: Preference Ranking Optimization for Human Alignment

The Capacity for Moral Self-Correction in Large Language Models

RAIN: Your Language Models Can Align Themselves without Finetuning



### Evaluation&Dataset

#### CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models

#### REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models



### value alignment

#### MEASURING VALUE UNDERSTANDING IN LANGUAGE MODELS THROUGH DISCRIMINATOR-CRITIQUE GAP

提出了value alignment不仅要考虑“know what”，还要考虑“know why”。

dataset：100个questions，每个question有10个answers，每个answer有1个reason，generated by GPT4。施瓦兹价值体系。

method：

discriminator：

给定一个question，让模型产生一个answer。

再给定该个question和该question对应10个value的answer，让模型选择哪个answer和它刚才的answer价值观最接近。（文章提出选择能力体现了理解能力）

用GPT4评判两个answer的相似度。

![image-20231025013015834](img/image-20231025013015834.png)

critique：

让模型输出选择的理由。

用GPT4评判模型输出的reason和选择的answer对应的reason的相似度。

![image-20231025013603163](img/image-20231025013603163.png)

最终结果

![image-20231025013624911](img/image-20231025013624911.png)

#### Heterogeneous Value Evaluation for Large Language Models

提出了value rationality，描述了agent在一个多元价值观中做出的决策使特定目标的价值最大化的能力。

建模公式：

![image-20231025013941319](img/image-20231025013941319.png)

最小化策略投影到value space和目标target在其中位置的距离。

用分硬币作为策略，映射后得到SVO值，再用SVO slider measure评定其value rationality。

![image-20231025014243242](img/image-20231025014243242.png)

![image-20231025014615632](img/image-20231025014615632.png)

![image-20231025014400045](img/image-20231025014400045.png)

问题：文章中的value space本质上是二维的，同时Vtarget也是人为定义的。价值体系简单，只包含了相对来说对立的价值。



### value theory

#### Schwartz 价值观理论的发展历程与最新进展

#### An Overview of the Schwartz Theory of Basic Values